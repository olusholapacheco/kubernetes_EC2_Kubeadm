name: Deploy Kubernetes Cluster using Kubeadm and AWS EC2s

on:
  push:
    branches:
      - main

jobs:
  deploy-infrastructure:
    runs-on: ubuntu-latest
    outputs:
      master_node_ip: ${{ steps.set-ips.outputs.master_node_ip }}
      worker_node_ips: ${{ steps.set-ips.outputs.worker_node_ips }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1
      with:
        terraform_version: 1.0.11

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Initialize Terraform
      run: terraform init -backend-config="bucket=kubernetes-kubeadm-bucket" -backend-config="key=kubernetes-kubeadm-bucket-key/terraform.tfstate" -backend-config="region=${{ secrets.AWS_REGION }}" -backend-config="dynamodb_table=terraform-lock"

    # Check if IAM Role is already imported; if not, import it
    - name: Check if IAM Role is already imported
      run: |
        terraform state show aws_iam_role.role || terraform import aws_iam_role.role S3_role
  
    # Check if IAM Instance Profile is already imported; if not, import it
    - name: Check if IAM Instance Profile is already imported
      run: |
        terraform state show aws_iam_instance_profile.s3_profile || terraform import aws_iam_instance_profile.s3_profile s3_profile
    
    # Check if IAM Policy is already imported; if not, import it
    # This prevents the policy duplication error
    - name: Check if IAM Policy is already imported
      run: |
        terraform state show aws_iam_policy.policy || terraform import aws_iam_policy.policy arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:policy/s3AccessPolicy
    
    - name: Apply Terraform
      id: apply
      run: terraform apply -auto-approve

    - name: Set master and worker node IPs
      id: set-ips
      run: |
        MASTER_NODE_IP=$(terraform output -raw master_public_ip 2>/dev/null)
        WORKER_NODE_IPS=$(terraform output -json worker_public_ips 2>/dev/null | jq -r '.[]')

        if [ -z "$MASTER_NODE_IP" ]; then
          echo "Error: Failed to retrieve master node IP."
          exit 1
        fi

        if [ -z "$WORKER_NODE_IPS" ]; then
          echo "Error: Failed to retrieve worker node IPs."
          exit 1
        fi

        echo "MASTER_NODE_IP=$MASTER_NODE_IP" >> $GITHUB_ENV
        echo "WORKER_NODE_IPS=$WORKER_NODE_IPS" >> $GITHUB_ENV
      outputs:
        master_node_ip: ${{ steps.set-ips.outputs.master_node_ip }}
        worker_node_ips: ${{ steps.set-ips.outputs.worker_node_ips }}

  setup-master-node:
    runs-on: ubuntu-latest
    needs: deploy-infrastructure

    steps:
    - name: Wait for master node to be ready
      run: |
        while ! ssh -o "StrictHostKeyChecking=no" ubuntu@$MASTER_NODE_IP "kubectl get nodes | grep Ready"; do
          echo "Waiting for master node to be ready..."
          sleep 10
        done

    - name: Run master node script
      run: |
        ssh -o "StrictHostKeyChecking=no" ubuntu@$MASTER_NODE_IP "sudo bash /root/master.sh"
      env:
        MASTER_NODE_IP: ${{ env.MASTER_NODE_IP }}

  setup-worker-nodes:
    runs-on: ubuntu-latest
    needs: deploy-infrastructure

    steps:
    - name: Run worker node script
      run: |
        for WORKER_NODE_IP in $WORKER_NODE_IPS; do
          ssh -o "StrictHostKeyChecking=no" ubuntu@$WORKER_NODE_IP "sudo bash /root/workers.sh"
        done
      env:
        WORKER_NODE_IPS: ${{ env.WORKER_NODE_IPS }}
